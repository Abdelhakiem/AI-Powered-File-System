{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "SUMMARIZATION_MODEL_NAME = \"facebook/bart-large-cnn\"\n",
        "SUMMARIZER_PATH = \"../models/summarizer/\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(SUMMARIZATION_MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(SUMMARIZATION_MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Save them locally\n",
        "tokenizer.save_pretrained(SUMMARIZER_PATH)\n",
        "model.save_pretrained(SUMMARIZER_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(SUMMARIZER_PATH)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(SUMMARIZER_PATH)\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "63kFp9L50im2"
      },
      "outputs": [],
      "source": [
        "def summarize_text_file(summarizer, text):\n",
        "    return summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ7N1XCt3jHr",
        "outputId": "2bbdd085-950e-41b8-eb86-fd2a1c0e33e4"
      },
      "outputs": [],
      "source": [
        "result = summarize_text_file(summarizer, \"\"\"When summarizing text files for the purpose of generating document embeddings, it's crucial to produce summaries that are both concise and representative of the original content. Here are some recommendations:\n",
        "\n",
        "Set do_sample=False: Ensures that the summary is consistent across runs, which is essential for reliable embeddings.\n",
        "\n",
        "Adjust max_length and min_length Based on Input Size:\n",
        "\n",
        "For shorter documents, you might set max_length=100 and min_length=30.\n",
        "\n",
        "For longer documents, consider increasing max_length to 200 or more, ensuring that the summary captures all key points.\n",
        "\n",
        "Consider Additional Parameters:\n",
        "\n",
        "num_beams: Setting this to a higher value (e.g., 4 or 5) can improve the quality of the summary by exploring more possible sequences during generation.\n",
        "\n",
        "length_penalty: Adjusting this can encourage the model to generate longer or shorter summaries. A value greater than 1.0 favors longer summaries, while a value less than 1.0 favors shorter ones.\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Set do_sample=False: Ensures that the summary is consistent across runs.Adjust max_length and min_length Based on Input Size: For shorter documents, you might set max_ lengths to 100 and min lengths to 30.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
