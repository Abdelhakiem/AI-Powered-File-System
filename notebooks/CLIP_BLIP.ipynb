{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForImageTextRetrieval, BlipForConditionalGeneration\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# ─── 0. CONFIGURATION & LOCAL PATHS ─────────────────────────────────────────────\n",
        "load_dotenv()  # Load variables from .env file\n",
        "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "if not HUGGINGFACE_TOKEN:\n",
        "    raise ValueError(\"Hugging Face token not found in .env file!\")\n",
        "login(token=HUGGINGFACE_TOKEN)\n",
        "\n",
        "\n",
        "\n",
        "CLIP_RETRIEVAL_ID = \"openai/clip-vit-base-patch32\"\n",
        "BLIP_CAPTION_ID   = \"Salesforce/blip-image-captioning-base\"\n",
        "\n",
        "# Paths where you previously saved the BLIP models & processors:\n",
        "LOCAL_CAPTION_DIR = \"../models/img_caption\"\n",
        "LOCAL_RETRIEVAL_DIR    = \"../models/embedding\"\n",
        "# Choose device (GPU if available):\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17b53e49511147cf802410c7004e4c2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34676f5fef2749c1950cf7783cea0086",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbafcbdecdbb415f9db0ed13aaa025bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "726d0b70a8214a5b8792a69507555518",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d34bba90277d4fb88f1271cc87df4be1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83bd30a254b14ce390e37389a94d1aa0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fa256e7f5b64e76883437de6aa21eef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d516c8233dbb42a5a9ea850f60e2a8b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# 2.2 Load BLIP captioning model + processor (for image→caption)\n",
        "caption_processor = BlipProcessor.from_pretrained(BLIP_CAPTION_ID)\n",
        "caption_model     = BlipForConditionalGeneration.from_pretrained(BLIP_CAPTION_ID)\n",
        "\n",
        "# Save the caption processor and model locally\n",
        "caption_processor.save_pretrained(LOCAL_CAPTION_DIR)\n",
        "caption_model.save_pretrained(LOCAL_CAPTION_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cd6f008730d44d89e0936a6eba962a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53c6ed8039694e4b9fa688f915402c72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e26cee1ebce448eb78f245a58b6c6d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1213db9993a4de480618b851d3ffca9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "808bbd0afae8412a91f33bdfc4f3895a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5354028ca8b4e5ebf2a11b1e0b40bbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "266cfaf498f5441585131fdf4fa98ca5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7a9fe6b95fc425db9b57604685f419a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e4a92db16f74d8fab14606f207d315a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "# ─── 2. LOAD FROM HUGGING FACE & SAVE LOCALLY ───────────────────────────────────\n",
        "# 2.1 Load BLIP retrieval model + processor (for text→embed & image→embed)\n",
        "retrieval_processor = CLIPModel.from_pretrained(CLIP_RETRIEVAL_ID)\n",
        "retrieval_model     = CLIPProcessor.from_pretrained(CLIP_RETRIEVAL_ID)\n",
        "\n",
        "# Save the retrieval processor and model locally\n",
        "retrieval_processor.save_pretrained(LOCAL_RETRIEVAL_DIR)\n",
        "retrieval_model.save_pretrained(LOCAL_RETRIEVAL_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 Load BLIP captioning model + processor (for image→text)\n",
        "caption_processor = BlipProcessor.from_pretrained(LOCAL_CAPTION_DIR)\n",
        "caption_model     = BlipForConditionalGeneration.from_pretrained(LOCAL_CAPTION_DIR).to(device)\n",
        "\n",
        "# 1.2 Load BLIP retrieval model + processor (for text→embed and image→embed)\n",
        "emb_processor = CLIPProcessor.from_pretrained(LOCAL_RETRIEVAL_DIR)\n",
        "emb_model     = CLIPModel.from_pretrained(LOCAL_RETRIEVAL_DIR).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 Load BLIP captioning model + processor (for image→text)\n",
        "caption_processor = BlipProcessor.from_pretrained(LOCAL_CAPTION_DIR)\n",
        "caption_model     = BlipForConditionalGeneration.from_pretrained(LOCAL_CAPTION_DIR).to(device)\n",
        "\n",
        "# 1.2 Load BLIP retrieval model + processor (for text→embed and image→embed)\n",
        "emb_processor = CLIPProcessor.from_pretrained(LOCAL_RETRIEVAL_DIR)\n",
        "emb_model     = CLIPModel.from_pretrained(LOCAL_RETRIEVAL_DIR).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# 1. Text to normalized vector (using CLIP)\n",
        "def text_to_vector(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert text to normalized L2 vector using CLIP\n",
        "    \n",
        "    Args:\n",
        "        text: Input text string\n",
        "        \n",
        "    Returns:\n",
        "        Normalized embedding vector (numpy array)\n",
        "    \"\"\"\n",
        "    inputs = emb_processor(text=text, return_tensors=\"pt\", padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = emb_model.get_text_features(**inputs)\n",
        "    # Normalize to unit vector (L2 norm)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    return text_features.cpu().numpy().squeeze()\n",
        "\n",
        "# 2. Image to normalized vector (using CLIP)\n",
        "def image_to_vector(image: Image.Image) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert image to normalized L2 vector using CLIP\n",
        "    \n",
        "    Args:\n",
        "        image: PIL Image object\n",
        "        \n",
        "    Returns:\n",
        "        Normalized embedding vector (numpy array)\n",
        "    \"\"\"\n",
        "    inputs = emb_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = emb_model.get_image_features(**inputs)\n",
        "    # Normalize to unit vector (L2 norm)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    return image_features.cpu().numpy().squeeze()\n",
        "\n",
        "# 3. Image to caption (using BLIP)\n",
        "def image_to_caption(image: Image.Image, max_length: int = 30) -> str:\n",
        "    \"\"\"\n",
        "    Generate caption from image using BLIP\n",
        "    \n",
        "    Args:\n",
        "        image: PIL Image object\n",
        "        max_length: Maximum caption length (default 30)\n",
        "        \n",
        "    Returns:\n",
        "        Generated caption string\n",
        "    \"\"\"\n",
        "    inputs = caption_processor(image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        output = caption_model.generate(**inputs, max_length=max_length)\n",
        "    caption = caption_processor.decode(output[0], skip_special_tokens=True)\n",
        "    return caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text embedding shape: (512,)\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Embed some text\n",
        "txt1 = \"banana holding gun with sunglasses\"\n",
        "txt1_embed = text_to_vector(txt1)\n",
        "print(\"Text embedding shape:\", txt1_embed.shape)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text embedding shape: (512,)\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Embed some text\n",
        "txt2 = \"t-test in hypothesis testing assumes nomality assumption of population distribution, if sample size is less than 30\"\n",
        "txt2_embed = text_to_vector(txt2)\n",
        "print(\"Text embedding shape:\", txt2_embed.shape)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated caption: a banana with sunglasses and a gun\n",
            "Image embedding shape: (512,)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img_path = \"image.png\"\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "img_embed = image_to_vector(image)\n",
        "caption = image_to_caption(image)\n",
        "print(\"Generated caption:\", caption)\n",
        "print(\"Image embedding shape:\", img_embed.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2h2kvfb89bs",
        "outputId": "d61bc77b-a4bb-4778-88e6-483439d6ad5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.35700822"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.17156272"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def find_similarity(emb1,emb2):\n",
        "    return np.dot(emb1, emb2)\n",
        "\n",
        "display(\n",
        "    find_similarity(txt1_embed, img_embed),\n",
        "    find_similarity(txt2_embed, img_embed),\n",
        "\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
